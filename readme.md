# Awesome-3d-diffusion-for-human-motion

## Diffusion for Motion

### Human Motion
- [Single Motion Diffusion](https://sinmdm.github.io/SinMDM-page/), Raab et al., ICLR 2024
- [OmniControl: Control Any Joint at Any Time for Human Motion Generation](https://arxiv.org/abs/2310.08580), Xie et al., ICLR 2024
- [Human Motion Diffusion as a Generative Prior](https://priormdm.github.io/priorMDM-page/), Shafir et al., ICLR 2024
- [MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation](https://arxiv.org/abs/2401.11115), Hoang et al., AAAI 2024
- [DNO: Optimizing Diffusion Noise Can Serve As Universal Motion Priors](https://korrawe.github.io/dno-project/), Karunratanakul et al., Arxiv 2023
- [RoHM: Robust Human Motion Reconstruction via Diffusion](https://arxiv.org/pdf/2401.08570.pdf), Zhang et al., Arxiv 2023
- [EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation](https://frank-zy-dou.github.io/projects/EMDM/index.html), Zhou et al., Arxiv 2023
- [DiffusionPhase: Motion Diffusion in Frequency Domain](https://arxiv.org/abs/2312.04036), Wan et al., Arxiv 2023
- [InterControl: Generate Human Motion Interactions by Controlling Every Joint](https://arxiv.org/abs/2311.15864), Wang et al., Arxiv 2023
- [AAMDM: Accelerated Auto-regressive Motion Diffusion Model](https://arxiv.org/abs/2401.06146), Li et al., Arxiv 2023
- [ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions](https://vcai.mpi-inf.mpg.de/projects/remos/), Ghosh et al., Arxiv 2023
- [HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models](https://arxiv.org/abs/2312.06553), Peng et al., Arxiv 2023
- [Controllable Motion Diffusion Model](https://arxiv.org/abs/2306.00416), Shi et al., Arxiv 2023
- [MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion](https://arxiv.org/abs/2310.14729), Kapon et al., Arxiv 2023
- [CG-HOI: Contact-Guided 3D Human-Object Interaction Generation](https://arxiv.org/pdf/2311.16097.pdf), Diller et al., Arxiv 2023
- [A Unified Framework for Multimodal, Multi-Part Human Motion Synthesis](https://arxiv.org/pdf/2311.16471.pdf), Zhou et al., Arxiv 2023
- [Controllable Motion Synthesis and Reconstruction with Autoregressive Diffusion Models](https://arxiv.org/abs/2304.04681), Yin et al., Arxiv 2023
- [Guided Motion Diffusion for Controllable Human Motion Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.pdf), Karunratanakul et al., ICCV 2023
- [Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models](https://openaccess.thecvf.com/content/ICCV2023/html/Pi_Hierarchical_Generation_of_Human-Object_Interactions_with_Diffusion_Probabilistic_Models_ICCV_2023_paper.html), Pi et al., ICCV 2023
- [PhysDiff: Physics-Guided Human Motion Diffusion Model](https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.pdf), Yuan et al., ICCV 2023
- [Priority-Centric Human Motion Generation in Discrete Latent Space](https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.pdf), Kong et al., ICCV 2023
- [ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf), Zhang et al., ICCV 2023
- [Flame: Free-form language-based motion synthesis & editing](https://ojs.aaai.org/index.php/AAAI/article/view/25996), Kim et al., AAAI 2023
- [Object Motion Guided Human Motion Synthesis](https://arxiv.org/abs/2309.16237), Li et al., SIG ASIA 2023, TOG 2023
- [Controllable Group Choreography using Contrastive Diffusion](https://arxiv.org/abs/2310.18986), Le et al., TOG 2023
- [Listen, denoise, action! Audio-driven motion synthesis with diffusion models](https://arxiv.org/abs/2211.09707), Alexanderson et al., SIG 2023, TOG 2023
- [GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents](https://aubrey-ao.github.io/publication/gesturediffuclip/) Ao et al., SIG 2023, TOG 2023
- [Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf), Zhu et al., CVPR 2023
- [MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf), Dabral et al., CVPR 2023
- [Executing your Commands via Motion Diffusion in Latent Space](https://arxiv.org/abs/2212.04048), Jiang et al., CVPR 2023
- [MDM: Human Motion Diffusion Model](https://guytevet.github.io/mdm-page/), Tevet et al., ICLR 2023
- [MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html), Zhang et al., Arxiv 2022


See [Awesome-3d-diffusion](https://github.com/cwchenwang/awesome-3d-diffusion)

# Motion_Tools
Some motion tools, maintained by [Frank Zhiyang Dou](https://frank-zy-dou.github.io/index.html).

A list of our research works on character animation (both kinematics-based and physics-based animation).
- **[Physically Simulated Character Animation]** 
  - CÂ·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters, SIGGRAPH ASIA 2023. [[Project Page]](https://frank-zy-dou.github.io/projects/CASE/index.html)
  - ModSkill: Physical Character Skill Modularization. Axiv 2025. [[Paper]](https://arxiv.org/html/2502.14140v1)
- **[Kinematics-based Character Animation]** 
  - TLcontrol: Trajectory and Language Control for Human Motion Synthesis, ECCV 2024. [[Project Page]](https://tlcontrol.weilinwl.com/)
  - EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation, ECCV 2024. [[Project Page]](https://frank-zy-dou.github.io/projects/EMDM/index.html)
- **[Crowd Animation]**
  - CBIL: Collective Behavior Imitation Learning for Fish from Real Videos, SIGGRAPH ASIA 2024. [[Project Page]](https://frank-zy-dou.github.io/projects/CBIL/index.html)

# Introduction
Tool list
- bvh2fbx: [bvh2fbx.py](bvh2fbx.py)
- smpl npy2mp4: [vis_tool_mp4.py](npy2mp4%2Fvis_tool_mp4.py)
- robot control via Vicon Mocap System: [Robot_Control](Robot_Control)
# Install
```angular2svg
conda create -n Motion_Tool python=3.8
conda activate Motion_Tool
pip install smplx
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117
pip install numpy
pip install matplotlib==3.1.3
pip install scipy
pip install scikit-image
pip install chumpy
pip install numpy==1.23.1
```
# Other links
- https://github.com/KosukeFukazawa/CharacterAnimationTools
- https://github.com/GuyTevet/motion-diffusion-model